#!/usr/bin/env bash

set -x

yes 'will cite' | parallel --bibtex > /dev/null 2>&1

osd_file=`mktemp`

ceph-disk list --format json \
| jq -r '.[].partitions[]? | select(.journal_dev!=null and .fs_type!="LVM2_member") | [.whoami,.path,.journal_dev] | join("\t")' \
| sort -k3 > $osd_file

ssds=(`cat $osd_file | awk '{ print $3 }' | sed 's/.$//' | uniq`)
ssd_num=${#ssds[*]}
hdd_num=`wc -l < $osd_file`
let "partitions_per_ssd=($hdd_num+$ssd_num-1)/$ssd_num" #ceil(hdd_num/ssd_num)

if [[ $partitions_per_ssd == 1 ]]; then
  echo "Machine has no journals"
  exit 1
fi

let "ssd_partition_size=100/$partitions_per_ssd"

out_and_destroy()
{
  osd_id=$1
  osd_block=$(echo $2 | sed 's/.$//')

  if [[ -z $osd_id  ]]; then
    return 1
  fi

  ceph osd out $osd_id
  while ! ceph osd safe-to-destroy  $osd_id; do sleep 30 ; done
  systemctl stop ceph-osd@$osd_id

  sleep 10

  if mount | grep -qP "/var/lib/ceph/osd/ceph-$osd_id\b"
  then
    umount -l /var/lib/ceph/osd/ceph-$osd_id
  fi

  ceph osd destroy $osd_id --yes-i-really-mean-it
  ceph-volume lvm zap $osd_block
}
export -f out_and_destroy

create_new_osd()
{
  osd_id=$1
  hdd_block=$(echo $2 | sed 's/.$//')
  ssd_block=$4

  if [[ -z $osd_id  ]]; then
    return 1
  fi

  ceph-volume lvm create --bluestore --osd-id $osd_id --data $hdd_block --block.db $ssd_block
}
export -f create_new_osd

for ssd in ${ssds[*]};
do
  grep $ssd $osd_file | parallel -C '\t' out_and_destroy

  dd if=/dev/zero of=$ssd bs=512 count=1
  parted -s $ssd mklabel gpt
  for i in `seq 0 $(($partitions_per_ssd-1))`;
  do
    parted -s -a optimal $ssd mkpart primary \
                      "$((i*$ssd_partition_size))%" \
                      "$(((i+1)*$ssd_partition_size))%"
  done
  parallel -C '\t' --xapply create_new_osd :::: <(grep $ssd $osd_file) ::: `seq 1 $partitions_per_ssd | sed "s|^|$ssd|"`
done

rm -f $osd_file
